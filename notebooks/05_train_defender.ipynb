{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2c0e2f49",
   "metadata": {},
   "source": [
    "Preparación del entorno"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "63108402",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import joblib\n",
    "from envs.multi_step_attack_env import AttackEnvMultiStep\n",
    "from stable_baselines3.common.callbacks import BaseCallback\n",
    "from envs.defender_env import DefenderEnv\n",
    "from stable_baselines3.common.vec_env import DummyVecEnv\n",
    "from stable_baselines3 import PPO, SAC"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "351aef8b",
   "metadata": {},
   "source": [
    "Funciones auxiliares"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0427c263",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DefenderLoggerCallback(BaseCallback):\n",
    "\n",
    "    def __init__(self, log_freq: int = 100, verbose: int = 0):\n",
    "        super().__init__(verbose)\n",
    "        self.log_freq = log_freq\n",
    "        self.episode_count = 0\n",
    "\n",
    "        # Buffers por ventana\n",
    "        self.rewards_buffer = []\n",
    "        self.det_rate_buffer = []\n",
    "        self.tn_rate_buffer = []\n",
    "        self.threshold_buffer = []\n",
    "\n",
    "        # Histórico\n",
    "        self.history_episodes = []\n",
    "        self.history_mean_reward = []\n",
    "        self.history_det_rate = []\n",
    "        self.history_tn_rate = []\n",
    "        self.history_mean_threshold = []\n",
    "\n",
    "        # Estado interno del episodio\n",
    "        self._reset_ep_stats()\n",
    "\n",
    "    def _reset_ep_stats(self):\n",
    "        self.ep_reward = 0.0\n",
    "        self.ep_steps = 0\n",
    "        self.ep_attacks = 0\n",
    "        self.ep_attacks_correct = 0\n",
    "        self.ep_normals = 0\n",
    "        self.ep_normals_correct = 0\n",
    "        self.ep_thresh_sum = 0.0\n",
    "\n",
    "\n",
    "    def _on_step(self) -> bool:\n",
    "\n",
    "        infos = self.locals[\"infos\"]\n",
    "        rewards = self.locals[\"rewards\"]\n",
    "        dones = self.locals[\"dones\"]\n",
    "\n",
    "        for info, r, done in zip(infos, rewards, dones):\n",
    "\n",
    "            # Acumular reward\n",
    "            self.ep_reward += float(r)\n",
    "            self.ep_steps += 1\n",
    "\n",
    "            y = info.get(\"y_sample\")\n",
    "            pred = info.get(\"pred\")\n",
    "            th = info.get(\"threshold\")\n",
    "\n",
    "            if y is not None and pred is not None:\n",
    "                if y == 1:\n",
    "                    self.ep_attacks += 1\n",
    "                    if pred == 1:\n",
    "                        self.ep_attacks_correct += 1\n",
    "                else:\n",
    "                    self.ep_normals += 1\n",
    "                    if pred == 0:\n",
    "                        self.ep_normals_correct += 1\n",
    "\n",
    "            if th is not None:\n",
    "                self.ep_thresh_sum += float(th)\n",
    "\n",
    "            # Cuando termina el episodio\n",
    "            if done:\n",
    "                self.episode_count += 1\n",
    "\n",
    "                # Métricas del episodio\n",
    "                det_rate = (self.ep_attacks_correct / self.ep_attacks) if self.ep_attacks > 0 else 0.0\n",
    "                tn_rate = (self.ep_normals_correct / self.ep_normals) if self.ep_normals > 0 else 0.0\n",
    "                mean_th = self.ep_thresh_sum / self.ep_steps\n",
    "\n",
    "                # Añadir a buffers\n",
    "                self.rewards_buffer.append(self.ep_reward)\n",
    "                self.det_rate_buffer.append(det_rate)\n",
    "                self.tn_rate_buffer.append(tn_rate)\n",
    "                self.threshold_buffer.append(mean_th)\n",
    "\n",
    "                # LOG cada ventana\n",
    "                if self.episode_count % self.log_freq == 0:\n",
    "\n",
    "                    mean_reward = np.mean(self.rewards_buffer)\n",
    "                    mean_det = np.mean(self.det_rate_buffer)\n",
    "                    mean_tn = np.mean(self.tn_rate_buffer)\n",
    "                    mean_th = np.mean(self.threshold_buffer)\n",
    "\n",
    "                    # Guardar en histórico\n",
    "                    self.history_episodes.append(self.episode_count)\n",
    "                    self.history_mean_reward.append(mean_reward)\n",
    "                    self.history_det_rate.append(mean_det)\n",
    "                    self.history_tn_rate.append(mean_tn)\n",
    "                    self.history_mean_threshold.append(mean_th)\n",
    "\n",
    "                    # Log bonito\n",
    "                    print(\n",
    "                        f\"[DefenderLogger] Episodios: {self.episode_count} | \"\n",
    "                        f\"Reward medio: {mean_reward:.3f} | \"\n",
    "                        f\"Detección ataques: {mean_det*100:.1f}% | \"\n",
    "                        f\"Acierto normales: {mean_tn*100:.1f}% | \"\n",
    "                        f\"Threshold medio: {mean_th:.3f}\"\n",
    "                    )\n",
    "\n",
    "                    # limpiar buffers\n",
    "                    self.rewards_buffer.clear()\n",
    "                    self.det_rate_buffer.clear()\n",
    "                    self.tn_rate_buffer.clear()\n",
    "                    self.threshold_buffer.clear()\n",
    "\n",
    "                # reset stats episodio\n",
    "                self._reset_ep_stats()\n",
    "\n",
    "        return True\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5eae7ec0",
   "metadata": {},
   "source": [
    "Creación del área de trabajo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c0f18b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Normal: (800, 2) Attack: (800, 2)\n"
     ]
    }
   ],
   "source": [
    "# Cargar datos\n",
    "data = np.load(\"../data/synthetic_2d.npz\")\n",
    "X_train, X_test = data[\"X_train\"], data[\"X_test\"]\n",
    "y_train, y_test = data[\"y_train\"], data[\"y_test\"]\n",
    "\n",
    "# Cargamos las muestras normales y de ataque\n",
    "normal_samples = X_train[y_train == 0]\n",
    "attack_samples = X_train[y_train == 1]\n",
    "\n",
    "# Imprimir formas de los datos cargados\n",
    "print(\"Normal:\", normal_samples.shape, \"Attack:\", attack_samples.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55eea8bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cargar clasificador clásico\n",
    "clf = joblib.load(\"../classifiers/logreg_synthetic_2d.joblib\")\n",
    "\n",
    "# Crear AttackEnvMultiStep igual que lo hicmos para entrenar SAC\n",
    "attack_env = AttackEnvMultiStep(\n",
    "    attack_samples=attack_samples,\n",
    "    clf=clf,\n",
    "    threshold=0.5,   # Este threshold solo afecta a la reward del atacante, no al defensor\n",
    "    epsilon=0.7,\n",
    "    penalty=0.01,\n",
    "    max_steps=5,\n",
    ")\n",
    "\n",
    "# Cargar modelo SAC atacante\n",
    "attacker_model = SAC.load(\"../agents/sac_attacker_multistep\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e9e4f42",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Función para crear el entorno del defensor\n",
    "def make_defender_env():\n",
    "    return DefenderEnv(\n",
    "        normal_samples=normal_samples,\n",
    "        attack_env=attack_env,\n",
    "        attacker_model=attacker_model,\n",
    "        init_threshold=0.5,\n",
    "        delta_max=0.1,\n",
    "        min_threshold=0.05,\n",
    "        max_threshold=0.95,\n",
    "        attack_prob=0.5,        # mitad normal / mitad ataque\n",
    "        episode_length=50,\n",
    "        extremal_penalty=0.1,\n",
    "    )\n",
    "\n",
    "# Crear entorno vectorizado para el defensor\n",
    "venv_def = DummyVecEnv([make_defender_env])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb555028",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[DefenderLogger] Episodios: 100 | Reward medio: -39.643 | Detección ataques: 1.2% | Acierto normales: 99.2% | Threshold medio: 0.484\n",
      "[DefenderLogger] Episodios: 200 | Reward medio: -38.690 | Detección ataques: 1.2% | Acierto normales: 99.0% | Threshold medio: 0.447\n",
      "[DefenderLogger] Episodios: 300 | Reward medio: -41.617 | Detección ataques: 1.5% | Acierto normales: 98.9% | Threshold medio: 0.466\n",
      "[DefenderLogger] Episodios: 400 | Reward medio: -40.107 | Detección ataques: 0.7% | Acierto normales: 98.9% | Threshold medio: 0.526\n",
      "[DefenderLogger] Episodios: 500 | Reward medio: -38.919 | Detección ataques: 0.9% | Acierto normales: 99.4% | Threshold medio: 0.596\n",
      "[DefenderLogger] Episodios: 600 | Reward medio: -39.866 | Detección ataques: 0.6% | Acierto normales: 99.4% | Threshold medio: 0.648\n",
      "[DefenderLogger] Episodios: 700 | Reward medio: -38.708 | Detección ataques: 0.7% | Acierto normales: 99.1% | Threshold medio: 0.629\n"
     ]
    }
   ],
   "source": [
    "policy_kwargs = dict(net_arch=[128, 128])\n",
    "\n",
    "defender_model = PPO(\n",
    "    \"MlpPolicy\",\n",
    "    venv_def,\n",
    "    verbose=0,\n",
    "    learning_rate=3e-4,\n",
    "    n_steps=2048,\n",
    "    batch_size=64,\n",
    "    policy_kwargs=policy_kwargs,\n",
    ")\n",
    "\n",
    "callback = DefenderLoggerCallback(log_freq=100)\n",
    "\n",
    "defender_model.learn(total_timesteps=100_000, callback=callback)\n",
    "defender_model.save(\"../agents/ppo_defender_threshold\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "77357417",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tasa detección ataques: 0.008\n",
      "Tasa acierto en tráfico normal: 0.992\n"
     ]
    }
   ],
   "source": [
    "def evaluate_defender(model, env, n_episodes=200):\n",
    "    correct_attacks = 0\n",
    "    total_attacks = 0\n",
    "    correct_normals = 0\n",
    "    total_normals = 0\n",
    "\n",
    "    obs = env.reset()\n",
    "    for _ in range(n_episodes):\n",
    "        done = False\n",
    "        truncated = False\n",
    "        while not (done or truncated):\n",
    "            action, _ = model.predict(obs, deterministic=True)\n",
    "            obs, reward, done, info = env.step(action)\n",
    "            info0 = info[0]\n",
    "\n",
    "            y = info0[\"y_sample\"]\n",
    "            pred = info0[\"pred\"]\n",
    "\n",
    "            if y == 1:\n",
    "                total_attacks += 1\n",
    "                if pred == 1:\n",
    "                    correct_attacks += 1\n",
    "            else:\n",
    "                total_normals += 1\n",
    "                if pred == 0:\n",
    "                    correct_normals += 1\n",
    "\n",
    "            truncated = info0.get(\"step_count\", 0) >= env.get_attr(\"episode_length\")[0]\n",
    "\n",
    "        obs = env.reset()\n",
    "\n",
    "    det_rate = correct_attacks / total_attacks if total_attacks > 0 else 0.0\n",
    "    tn_rate = correct_normals / total_normals if total_normals > 0 else 0.0\n",
    "    return det_rate, tn_rate\n",
    "\n",
    "det_rate, tn_rate = evaluate_defender(defender_model, venv_def, n_episodes=200)\n",
    "\n",
    "print(f\"Tasa detección ataques: {det_rate:.3f}\")\n",
    "print(f\"Tasa acierto en tráfico normal: {tn_rate:.3f}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tesis-roberto",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
