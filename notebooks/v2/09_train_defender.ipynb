{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2c0e2f49",
   "metadata": {},
   "source": [
    "Preparación del entorno"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "63108402",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import joblib\n",
    "from envs.v2.multi_step_limited_attack_env import AttackEnvLimitedMultiStep\n",
    "from stable_baselines3.common.callbacks import BaseCallback\n",
    "from envs.v2.defender_env import DefenderEnv\n",
    "from stable_baselines3.common.vec_env import DummyVecEnv\n",
    "from stable_baselines3 import PPO, SAC"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "351aef8b",
   "metadata": {},
   "source": [
    "Funciones auxiliares"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0427c263",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DefenderLoggerCallback(BaseCallback):\n",
    "\n",
    "    def __init__(self, log_freq: int = 100, verbose: int = 0):\n",
    "        super().__init__(verbose)\n",
    "        self.log_freq = log_freq\n",
    "        self.episode_count = 0\n",
    "\n",
    "        # Buffers por ventana\n",
    "        self.rewards_buffer = []\n",
    "        self.det_rate_buffer = []\n",
    "        self.tn_rate_buffer = []\n",
    "        self.threshold_buffer = []\n",
    "\n",
    "        # Histórico\n",
    "        self.history_episodes = []\n",
    "        self.history_mean_reward = []\n",
    "        self.history_det_rate = []\n",
    "        self.history_tn_rate = []\n",
    "        self.history_mean_threshold = []\n",
    "\n",
    "        # Estado interno del episodio\n",
    "        self._reset_ep_stats()\n",
    "\n",
    "    def _reset_ep_stats(self):\n",
    "        self.ep_reward = 0.0\n",
    "        self.ep_steps = 0\n",
    "        self.ep_attacks = 0\n",
    "        self.ep_attacks_correct = 0\n",
    "        self.ep_normals = 0\n",
    "        self.ep_normals_correct = 0\n",
    "        self.ep_thresh_sum = 0.0\n",
    "\n",
    "\n",
    "    def _on_step(self) -> bool:\n",
    "\n",
    "        infos = self.locals[\"infos\"]\n",
    "        rewards = self.locals[\"rewards\"]\n",
    "        dones = self.locals[\"dones\"]\n",
    "\n",
    "        for info, r, done in zip(infos, rewards, dones):\n",
    "\n",
    "            # Acumular reward\n",
    "            self.ep_reward += float(r)\n",
    "            self.ep_steps += 1\n",
    "\n",
    "            y = info.get(\"y_sample\")\n",
    "            pred = info.get(\"pred\")\n",
    "            th = info.get(\"threshold\")\n",
    "\n",
    "            if y is not None and pred is not None:\n",
    "                if y == 1:\n",
    "                    self.ep_attacks += 1\n",
    "                    if pred == 1:\n",
    "                        self.ep_attacks_correct += 1\n",
    "                else:\n",
    "                    self.ep_normals += 1\n",
    "                    if pred == 0:\n",
    "                        self.ep_normals_correct += 1\n",
    "\n",
    "            if th is not None:\n",
    "                self.ep_thresh_sum += float(th)\n",
    "\n",
    "            # Cuando termina el episodio\n",
    "            if done:\n",
    "                self.episode_count += 1\n",
    "\n",
    "                # Métricas del episodio\n",
    "                det_rate = (self.ep_attacks_correct / self.ep_attacks) if self.ep_attacks > 0 else 0.0\n",
    "                tn_rate = (self.ep_normals_correct / self.ep_normals) if self.ep_normals > 0 else 0.0\n",
    "                mean_th = self.ep_thresh_sum / self.ep_steps\n",
    "\n",
    "                # Añadir a buffers\n",
    "                self.rewards_buffer.append(self.ep_reward)\n",
    "                self.det_rate_buffer.append(det_rate)\n",
    "                self.tn_rate_buffer.append(tn_rate)\n",
    "                self.threshold_buffer.append(mean_th)\n",
    "\n",
    "                # LOG cada ventana\n",
    "                if self.episode_count % self.log_freq == 0:\n",
    "\n",
    "                    mean_reward = np.mean(self.rewards_buffer)\n",
    "                    mean_det = np.mean(self.det_rate_buffer)\n",
    "                    mean_tn = np.mean(self.tn_rate_buffer)\n",
    "                    mean_th = np.mean(self.threshold_buffer)\n",
    "\n",
    "                    # Guardar en histórico\n",
    "                    self.history_episodes.append(self.episode_count)\n",
    "                    self.history_mean_reward.append(mean_reward)\n",
    "                    self.history_det_rate.append(mean_det)\n",
    "                    self.history_tn_rate.append(mean_tn)\n",
    "                    self.history_mean_threshold.append(mean_th)\n",
    "\n",
    "                    # Log bonito\n",
    "                    print(\n",
    "                        f\"[DefenderLogger] Episodios: {self.episode_count} | \"\n",
    "                        f\"Reward medio: {mean_reward:.3f} | \"\n",
    "                        f\"Detección ataques: {mean_det*100:.1f}% | \"\n",
    "                        f\"Acierto normales: {mean_tn*100:.1f}% | \"\n",
    "                        f\"Threshold medio: {mean_th:.3f}\"\n",
    "                    )\n",
    "\n",
    "                    # limpiar buffers\n",
    "                    self.rewards_buffer.clear()\n",
    "                    self.det_rate_buffer.clear()\n",
    "                    self.tn_rate_buffer.clear()\n",
    "                    self.threshold_buffer.clear()\n",
    "\n",
    "                # reset stats episodio\n",
    "                self._reset_ep_stats()\n",
    "\n",
    "        return True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5eae7ec0",
   "metadata": {},
   "source": [
    "Creación del área de trabajo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8c0f18b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Normal: (800, 2) Attack: (800, 2)\n"
     ]
    }
   ],
   "source": [
    "# Cargar datos\n",
    "data = np.load(\"../../data/v2/synthetic_2d.npz\")\n",
    "X_train, X_test = data[\"X_train\"], data[\"X_test\"]\n",
    "y_train, y_test = data[\"y_train\"], data[\"y_test\"]\n",
    "\n",
    "# Cargamos las muestras normales y de ataque\n",
    "normal_samples = X_train[y_train == 0]\n",
    "attack_samples = X_train[y_train == 1]\n",
    "\n",
    "# Imprimir formas de los datos cargados\n",
    "print(\"Normal:\", normal_samples.shape, \"Attack:\", attack_samples.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "55eea8bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cargar clasificador clásico\n",
    "clf = joblib.load(\"../../classifiers/v2/logreg_synthetic_2d.joblib\")\n",
    "\n",
    "# Crear AttackEnvLimitedMultiStep igual que lo hicmos para entrenar SAC\n",
    "attack_env = AttackEnvLimitedMultiStep(\n",
    "    attack_samples=attack_samples,\n",
    "    clf=clf,\n",
    "    threshold=0.5,   # Este threshold solo afecta a la reward del atacante, no al defensor\n",
    "    epsilon=0.7,\n",
    "    penalty=0.01,\n",
    "    max_steps=5,\n",
    ")\n",
    "\n",
    "# Cargar modelo SAC atacante\n",
    "attacker_model = SAC.load(\"../../agents/v2/sac_attacker_limited_multistep\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4e9e4f42",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Función para crear el entorno del defensor\n",
    "def make_defender_env():\n",
    "    return DefenderEnv(\n",
    "        normal_samples=normal_samples,\n",
    "        attack_env=attack_env,\n",
    "        attacker_model=attacker_model,\n",
    "        init_threshold=0.5,\n",
    "        delta_max=0.1,\n",
    "        min_threshold=0.05,\n",
    "        max_threshold=0.95,\n",
    "        attack_prob=0.5,        # mitad normal / mitad ataque\n",
    "        episode_length=50,\n",
    "        extremal_penalty=0.1,\n",
    "    )\n",
    "\n",
    "# Crear entorno vectorizado para el defensor\n",
    "venv_def = DummyVecEnv([make_defender_env])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fb555028",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[DefenderLogger] Episodios: 100 | Reward medio: 7.886 | Detección ataques: 39.3% | Acierto normales: 60.1% | Threshold medio: 0.417\n",
      "[DefenderLogger] Episodios: 200 | Reward medio: 15.316 | Detección ataques: 47.8% | Acierto normales: 47.9% | Threshold medio: 0.220\n",
      "[DefenderLogger] Episodios: 300 | Reward medio: 17.725 | Detección ataques: 52.0% | Acierto normales: 44.9% | Threshold medio: 0.149\n",
      "[DefenderLogger] Episodios: 400 | Reward medio: 17.339 | Detección ataques: 55.4% | Acierto normales: 44.0% | Threshold medio: 0.138\n",
      "[DefenderLogger] Episodios: 500 | Reward medio: 16.257 | Detección ataques: 53.2% | Acierto normales: 45.9% | Threshold medio: 0.143\n",
      "[DefenderLogger] Episodios: 600 | Reward medio: 16.936 | Detección ataques: 53.9% | Acierto normales: 44.3% | Threshold medio: 0.154\n",
      "[DefenderLogger] Episodios: 700 | Reward medio: 18.037 | Detección ataques: 55.2% | Acierto normales: 44.3% | Threshold medio: 0.140\n",
      "[DefenderLogger] Episodios: 800 | Reward medio: 17.475 | Detección ataques: 56.2% | Acierto normales: 42.9% | Threshold medio: 0.125\n",
      "[DefenderLogger] Episodios: 900 | Reward medio: 17.587 | Detección ataques: 58.0% | Acierto normales: 41.7% | Threshold medio: 0.114\n",
      "[DefenderLogger] Episodios: 1000 | Reward medio: 16.191 | Detección ataques: 52.1% | Acierto normales: 43.1% | Threshold medio: 0.133\n",
      "[DefenderLogger] Episodios: 1100 | Reward medio: 16.321 | Detección ataques: 53.9% | Acierto normales: 46.6% | Threshold medio: 0.147\n",
      "[DefenderLogger] Episodios: 1200 | Reward medio: 17.479 | Detección ataques: 54.2% | Acierto normales: 44.2% | Threshold medio: 0.145\n",
      "[DefenderLogger] Episodios: 1300 | Reward medio: 18.082 | Detección ataques: 54.5% | Acierto normales: 45.3% | Threshold medio: 0.140\n",
      "[DefenderLogger] Episodios: 1400 | Reward medio: 17.694 | Detección ataques: 53.5% | Acierto normales: 46.7% | Threshold medio: 0.137\n",
      "[DefenderLogger] Episodios: 1500 | Reward medio: 18.521 | Detección ataques: 55.4% | Acierto normales: 44.8% | Threshold medio: 0.135\n",
      "[DefenderLogger] Episodios: 1600 | Reward medio: 17.982 | Detección ataques: 56.6% | Acierto normales: 45.5% | Threshold medio: 0.134\n",
      "[DefenderLogger] Episodios: 1700 | Reward medio: 17.665 | Detección ataques: 53.2% | Acierto normales: 45.5% | Threshold medio: 0.137\n",
      "[DefenderLogger] Episodios: 1800 | Reward medio: 17.870 | Detección ataques: 53.0% | Acierto normales: 45.2% | Threshold medio: 0.132\n",
      "[DefenderLogger] Episodios: 1900 | Reward medio: 17.470 | Detección ataques: 56.0% | Acierto normales: 41.8% | Threshold medio: 0.119\n",
      "[DefenderLogger] Episodios: 2000 | Reward medio: 17.559 | Detección ataques: 55.4% | Acierto normales: 43.3% | Threshold medio: 0.110\n"
     ]
    }
   ],
   "source": [
    "# Arquitectura de la red neuronal para el defensor\n",
    "policy_kwargs = dict(net_arch=[128, 128])\n",
    "\n",
    "defender_model = PPO(\n",
    "    \"MlpPolicy\",\n",
    "    venv_def,\n",
    "    verbose=0,\n",
    "    learning_rate=3e-4,\n",
    "    n_steps=2048, # Número de pasos por actualización\n",
    "    batch_size=64, # Grupos en los que se divide cada actualización\n",
    "    policy_kwargs=policy_kwargs,\n",
    ")\n",
    "\n",
    "# Creación de callback personalizado para logging\n",
    "callback = DefenderLoggerCallback(log_freq=100)\n",
    "\n",
    "# Entrenamiento del modelo del defensor\n",
    "defender_model.learn(total_timesteps=100_000, callback=callback)\n",
    "\n",
    "# Guardar el modelo entrenado del defensor\n",
    "defender_model.save(\"../../agents/v2/ppo_defender_threshold\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77357417",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tasa detección ataques: 0.008\n",
      "Tasa acierto en tráfico normal: 0.992\n"
     ]
    }
   ],
   "source": [
    "def evaluate_defender(model, env, n_episodes=200):\n",
    "    correct_attacks = 0\n",
    "    total_attacks = 0\n",
    "    correct_normals = 0\n",
    "    total_normals = 0\n",
    "\n",
    "    obs = env.reset()\n",
    "    for _ in range(n_episodes):\n",
    "        done = False\n",
    "        truncated = False\n",
    "        while not (done or truncated):\n",
    "            action, _ = model.predict(obs, deterministic=True)\n",
    "            obs, reward, done, info = env.step(action)\n",
    "            info0 = info[0]\n",
    "\n",
    "            y = info0[\"y_sample\"]\n",
    "            pred = info0[\"pred\"]\n",
    "\n",
    "            if y == 1:\n",
    "                total_attacks += 1\n",
    "                if pred == 1:\n",
    "                    correct_attacks += 1\n",
    "            else:\n",
    "                total_normals += 1\n",
    "                if pred == 0:\n",
    "                    correct_normals += 1\n",
    "\n",
    "            truncated = info0.get(\"step_count\", 0) >= env.get_attr(\"episode_length\")[0]\n",
    "\n",
    "        obs = env.reset()\n",
    "\n",
    "    det_rate = correct_attacks / total_attacks if total_attacks > 0 else 0.0\n",
    "    tn_rate = correct_normals / total_normals if total_normals > 0 else 0.0\n",
    "    return det_rate, tn_rate\n",
    "\n",
    "det_rate, tn_rate = evaluate_defender(defender_model, venv_def, n_episodes=200)\n",
    "\n",
    "print(f\"Tasa detección ataques: {det_rate:.3f}\")\n",
    "print(f\"Tasa acierto en tráfico normal: {tn_rate:.3f}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tesis-roberto",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
