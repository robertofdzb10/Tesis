{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0709b0ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import joblib\n",
    "from stable_baselines3 import SAC\n",
    "from stable_baselines3.common.vec_env import DummyVecEnv, VecNormalize\n",
    "from stable_baselines3.common.callbacks import BaseCallback\n",
    "from envs.v2.defender_env_3 import DefenderEnvV3\n",
    "from envs.v2.multi_step_limited_attack_env import AttackEnvLimitedMultiStep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "16732b35",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from stable_baselines3.common.callbacks import BaseCallback\n",
    "\n",
    "\n",
    "class DefenderLoggerCallback(BaseCallback):\n",
    "    def __init__(self, log_freq: int = 1000, verbose: int = 0):\n",
    "        super().__init__(verbose)\n",
    "        self.log_freq = log_freq\n",
    "        self.episode_count = 0\n",
    "\n",
    "        # Buffers por ventana\n",
    "        self.rewards_buffer = []            # Recompensas de los pasos\n",
    "        self.threshold_buffer = []          # Threshold global\n",
    "        self.p_attack_buffer = []           # p_attack del clasificador base\n",
    "        self.adv_dist_buffer = []           # Distancia adversarial (||x_adv - x_orig||)\n",
    "        self.adv_prog_buffer = []           # Progreso del ataque (step/max_steps)\n",
    "        self.dist_center_buffer = []        # Distancia normalizada al centro del cluster normal\n",
    "        self.cluster_score_buffer = []      # 1/(1+dist_center_norm)\n",
    "\n",
    "        # Contadores para TPR / FPR\n",
    "        self.attack_count = 0               # Nº muestras de ataque\n",
    "        self.tp_count = 0                   # Ataques bien detectados\n",
    "        self.normal_count = 0               # Nº muestras normales\n",
    "        self.fp_count = 0                   # Falsos positivos\n",
    "\n",
    "        # Históricos para graficar luego\n",
    "        self.history_episodes = []\n",
    "        self.history_mean_reward = []\n",
    "        self.history_detection_rate = []      # TPR\n",
    "        self.history_false_positive_rate = [] # FPR\n",
    "        self.history_mean_threshold = []\n",
    "        self.history_mean_p_attack = []\n",
    "        self.history_mean_adv_dist = []\n",
    "        self.history_mean_adv_prog = []\n",
    "        self.history_mean_dist_center = []\n",
    "        self.history_mean_cluster_score = []\n",
    "\n",
    "    def _on_step(self) -> bool:\n",
    "\n",
    "        infos = self.locals[\"infos\"]\n",
    "        rewards = self.locals[\"rewards\"]\n",
    "        dones = self.locals[\"dones\"]\n",
    "\n",
    "        for info, r, done in zip(infos, rewards, dones):\n",
    "\n",
    "            # Reward del paso\n",
    "            self.rewards_buffer.append(float(r))\n",
    "\n",
    "            if isinstance(info, dict):\n",
    "\n",
    "                # Threshold global\n",
    "                if \"threshold\" in info:\n",
    "                    self.threshold_buffer.append(float(info[\"threshold\"]))\n",
    "\n",
    "                # Probabilidad de ataque del clasificador\n",
    "                if \"p_attack\" in info:\n",
    "                    self.p_attack_buffer.append(float(info[\"p_attack\"]))\n",
    "\n",
    "                # Extra de V3: distancia adversarial, progreso, etc.\n",
    "                if \"adv_distance\" in info:\n",
    "                    self.adv_dist_buffer.append(float(info[\"adv_distance\"]))\n",
    "\n",
    "                if \"adv_progress\" in info:\n",
    "                    self.adv_prog_buffer.append(float(info[\"adv_progress\"]))\n",
    "\n",
    "                if \"dist_center_norm\" in info:\n",
    "                    self.dist_center_buffer.append(float(info[\"dist_center_norm\"]))\n",
    "\n",
    "                if \"cluster_score\" in info:\n",
    "                    self.cluster_score_buffer.append(float(info[\"cluster_score\"]))\n",
    "\n",
    "                # Etiqueta real y predicción del defensor\n",
    "                if \"y_sample\" in info and \"pred\" in info:\n",
    "                    y = int(info[\"y_sample\"])   # 0 = normal, 1 = ataque\n",
    "                    pred = int(info[\"pred\"])    # 0 = normal, 1 = ataque\n",
    "\n",
    "                    if y == 1:\n",
    "                        self.attack_count += 1\n",
    "                        if pred == 1:\n",
    "                            self.tp_count += 1\n",
    "                    else:\n",
    "                        self.normal_count += 1\n",
    "                        if pred == 1:\n",
    "                            self.fp_count += 1\n",
    "\n",
    "            # Fin de episodio\n",
    "            if done:\n",
    "                self.episode_count += 1\n",
    "\n",
    "                if self.episode_count > 0 and self.episode_count % self.log_freq == 0:\n",
    "\n",
    "                    # Reward media por paso en la ventana\n",
    "                    mean_reward = np.mean(self.rewards_buffer) if self.rewards_buffer else 0.0\n",
    "\n",
    "                    # TPR/FPR\n",
    "                    detection_rate = (self.tp_count / self.attack_count) if self.attack_count > 0 else 0.0\n",
    "                    false_positive_rate = (self.fp_count / self.normal_count) if self.normal_count > 0 else 0.0\n",
    "\n",
    "                    # Stats de threshold y p_attack\n",
    "                    mean_threshold = np.mean(self.threshold_buffer) if self.threshold_buffer else 0.0\n",
    "                    mean_p_attack = np.mean(self.p_attack_buffer) if self.p_attack_buffer else 0.0\n",
    "\n",
    "                    # Stats extra V3\n",
    "                    mean_adv_dist = np.mean(self.adv_dist_buffer) if self.adv_dist_buffer else 0.0\n",
    "                    mean_adv_prog = np.mean(self.adv_prog_buffer) if self.adv_prog_buffer else 0.0\n",
    "                    mean_dist_center = np.mean(self.dist_center_buffer) if self.dist_center_buffer else 0.0\n",
    "                    mean_cluster_score = np.mean(self.cluster_score_buffer) if self.cluster_score_buffer else 0.0\n",
    "\n",
    "                    # Guardamos históricos\n",
    "                    self.history_episodes.append(self.episode_count)\n",
    "                    self.history_mean_reward.append(mean_reward)\n",
    "                    self.history_detection_rate.append(detection_rate)\n",
    "                    self.history_false_positive_rate.append(false_positive_rate)\n",
    "                    self.history_mean_threshold.append(mean_threshold)\n",
    "                    self.history_mean_p_attack.append(mean_p_attack)\n",
    "                    self.history_mean_adv_dist.append(mean_adv_dist)\n",
    "                    self.history_mean_adv_prog.append(mean_adv_prog)\n",
    "                    self.history_mean_dist_center.append(mean_dist_center)\n",
    "                    self.history_mean_cluster_score.append(mean_cluster_score)\n",
    "\n",
    "                    # Log por consola\n",
    "                    print(\n",
    "                        f\"[DefenderLoggerV3] Episodios: {self.episode_count:6d} | \"\n",
    "                        f\"Reward media: {mean_reward: .3f} | \"\n",
    "                        f\"TPR: {detection_rate*100:5.1f}% | \"\n",
    "                        f\"FPR: {false_positive_rate*100:5.1f}% | \"\n",
    "                        f\"th: {mean_threshold: .3f} | \"\n",
    "                        f\"p_attack: {mean_p_attack: .3f} | \"\n",
    "                        f\"adv_dist: {mean_adv_dist: .3f} | \"\n",
    "                        f\"adv_prog: {mean_adv_prog: .3f} | \"\n",
    "                        f\"dist_centro: {mean_dist_center: .3f}\"\n",
    "                    )\n",
    "\n",
    "                    # Reset buffers ventana\n",
    "                    self.rewards_buffer.clear()\n",
    "                    self.threshold_buffer.clear()\n",
    "                    self.p_attack_buffer.clear()\n",
    "                    self.adv_dist_buffer.clear()\n",
    "                    self.adv_prog_buffer.clear()\n",
    "                    self.dist_center_buffer.clear()\n",
    "                    self.cluster_score_buffer.clear()\n",
    "                    self.attack_count = 0\n",
    "                    self.tp_count = 0\n",
    "                    self.normal_count = 0\n",
    "                    self.fp_count = 0\n",
    "\n",
    "        return True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9e1f5218",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------------------------------------------\n",
    "# 2. Cargar dataset y atacante\n",
    "# ----------------------------------------------------------\n",
    "\n",
    "data = np.load(\"../../data/v2/synthetic_2d.npz\")\n",
    "X_train, X_test = data[\"X_train\"], data[\"X_test\"]\n",
    "y_train, _ = data[\"y_train\"], data[\"y_test\"]\n",
    "\n",
    "normal_samples = X_train[y_train == 0]\n",
    "attack_samples = X_train[y_train == 1]\n",
    "\n",
    "clf = joblib.load(\"../../classifiers/v2/logreg_synthetic_2d.joblib\")\n",
    "\n",
    "attack_env = AttackEnvLimitedMultiStep(\n",
    "    attack_samples=attack_samples,\n",
    "    clf=clf,\n",
    "    threshold=0.5,\n",
    "    epsilon=0.7,\n",
    "    penalty=0.01,\n",
    "    max_steps=5,\n",
    ")\n",
    "\n",
    "attacker_model = SAC.load(\"../../agents/v2/sac_attacker_limited_multistep\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "459ec138",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------------------------------------------\n",
    "# 3. Crear entorno DEFENSOR V3\n",
    "# ----------------------------------------------------------\n",
    "\n",
    "def make_defender_env():\n",
    "    return DefenderEnvV3(\n",
    "        normal_samples=normal_samples,\n",
    "        attack_env=attack_env,\n",
    "        attacker_model=attacker_model,\n",
    "        init_threshold=0.5,\n",
    "        delta_max=0.05,\n",
    "        min_threshold=0.2,\n",
    "        max_threshold=0.8,\n",
    "        attack_prob=0.3,\n",
    "        episode_length=80,\n",
    "        tp_reward=3.0,\n",
    "        fn_penalty=-6.0,\n",
    "        tn_reward=1.0,\n",
    "        fp_penalty=-2.0,\n",
    "        move_penalty=0.1,\n",
    "        sensitivity_penalty=0.05,\n",
    "        extreme_penalty=0.5,\n",
    "    )\n",
    "\n",
    "\n",
    "venv_def = DummyVecEnv([make_defender_env])\n",
    "\n",
    "# Normalizar observaciones\n",
    "venv_def = VecNormalize(venv_def, norm_obs=True, norm_reward=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "654aa3ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------------------------------------------\n",
    "# 4. SAC para el DEFENSOR V3\n",
    "# ----------------------------------------------------------\n",
    "\n",
    "policy_kwargs = dict(net_arch=[256, 256])\n",
    "\n",
    "model_defender = SAC(\n",
    "    \"MlpPolicy\",\n",
    "    venv_def,\n",
    "    learning_rate=3e-4,\n",
    "    buffer_size=400_000,\n",
    "    batch_size=256,\n",
    "    tau=0.02,\n",
    "    train_freq=1,\n",
    "    gradient_steps=1,\n",
    "    gamma=0.99,\n",
    "    policy_kwargs=policy_kwargs,\n",
    "    verbose=0,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "9ce2ed9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[DefenderLoggerV3] Episodios:    100 | Reward media:  0.734 | TPR:  27.4% | FPR:  28.0% | th:  0.400 | p_attack:  0.255 | adv_dist:  0.000 | adv_prog:  0.000 | dist_centro:  0.000\n",
      "[DefenderLoggerV3] Episodios:    200 | Reward media:  0.908 | TPR:  29.3% | FPR:  29.7% | th:  0.347 | p_attack:  0.255 | adv_dist:  0.000 | adv_prog:  0.000 | dist_centro:  0.000\n",
      "[DefenderLoggerV3] Episodios:    300 | Reward media:  0.937 | TPR:  29.5% | FPR:  31.6% | th:  0.347 | p_attack:  0.261 | adv_dist:  0.000 | adv_prog:  0.000 | dist_centro:  0.000\n",
      "[DefenderLoggerV3] Episodios:    400 | Reward media:  0.957 | TPR:  29.6% | FPR:  30.3% | th:  0.336 | p_attack:  0.254 | adv_dist:  0.000 | adv_prog:  0.000 | dist_centro:  0.000\n",
      "[DefenderLoggerV3] Episodios:    500 | Reward media:  0.991 | TPR:  30.8% | FPR:  30.1% | th:  0.336 | p_attack:  0.254 | adv_dist:  0.000 | adv_prog:  0.000 | dist_centro:  0.000\n",
      "[DefenderLoggerV3] Episodios:    600 | Reward media:  0.989 | TPR:  28.8% | FPR:  30.6% | th:  0.331 | p_attack:  0.254 | adv_dist:  0.000 | adv_prog:  0.000 | dist_centro:  0.000\n",
      "[DefenderLoggerV3] Episodios:    700 | Reward media:  1.016 | TPR:  32.3% | FPR:  30.3% | th:  0.325 | p_attack:  0.255 | adv_dist:  0.000 | adv_prog:  0.000 | dist_centro:  0.000\n",
      "[DefenderLoggerV3] Episodios:    800 | Reward media:  1.036 | TPR:  31.9% | FPR:  29.8% | th:  0.324 | p_attack:  0.256 | adv_dist:  0.000 | adv_prog:  0.000 | dist_centro:  0.000\n",
      "[DefenderLoggerV3] Episodios:    900 | Reward media:  1.005 | TPR:  31.3% | FPR:  29.3% | th:  0.323 | p_attack:  0.254 | adv_dist:  0.000 | adv_prog:  0.000 | dist_centro:  0.000\n",
      "[DefenderLoggerV3] Episodios:   1000 | Reward media:  1.047 | TPR:  30.3% | FPR:  30.6% | th:  0.325 | p_attack:  0.255 | adv_dist:  0.000 | adv_prog:  0.000 | dist_centro:  0.000\n",
      "[DefenderLoggerV3] Episodios:   1100 | Reward media:  0.993 | TPR:  31.1% | FPR:  31.3% | th:  0.327 | p_attack:  0.261 | adv_dist:  0.000 | adv_prog:  0.000 | dist_centro:  0.000\n",
      "[DefenderLoggerV3] Episodios:   1200 | Reward media:  1.008 | TPR:  30.9% | FPR:  29.7% | th:  0.324 | p_attack:  0.249 | adv_dist:  0.000 | adv_prog:  0.000 | dist_centro:  0.000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<stable_baselines3.sac.sac.SAC at 0x1510586aad0>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ----------------------------------------------------------\n",
    "# 5. Entrenamiento\n",
    "# ----------------------------------------------------------\n",
    "callback = DefenderLoggerCallback(log_freq=100)\n",
    "\n",
    "model_defender.learn(total_timesteps=100_000, callback=callback)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f8dea35",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Modelo V3 guardado correctamente.\n"
     ]
    }
   ],
   "source": [
    "# ----------------------------------------------------------\n",
    "# 6. Guardar modelo + normalizador\n",
    "# ----------------------------------------------------------\n",
    "os.makedirs(\"../../agents/v2/\", exist_ok=True)\n",
    "\n",
    "model_defender.save(\"../../agents/v2/sac_defender_3\")\n",
    "venv_def.save(\"../../agents/v2/sac_defender_3_vecnorm.pkl\")\n",
    "\n",
    "print(\"Modelo V3 guardado correctamente.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "e4dfb292",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== Evaluación rápida =====\n",
      "TPR: 31.25%\n",
      "FPR: 30.15%\n"
     ]
    }
   ],
   "source": [
    "# ----------------------------------------------------------\n",
    "# 7. Evaluación rápida\n",
    "# ----------------------------------------------------------\n",
    "\n",
    "# Preparar entorno para evaluación\n",
    "eval_env = DummyVecEnv([make_defender_env])\n",
    "eval_env = VecNormalize.load(\"../../agents/v2/sac_defender_3_vecnorm.pkl\", eval_env)\n",
    "eval_env.training = False\n",
    "eval_env.norm_reward = False\n",
    "\n",
    "obs = eval_env.reset()\n",
    "\n",
    "TP = 0\n",
    "FP = 0\n",
    "TA = 0  # ataques totales\n",
    "TN = 0\n",
    "NO = 0  # normales totales\n",
    "\n",
    "for _ in range(200):\n",
    "    action, _ = model_defender.predict(obs, deterministic=True)\n",
    "    obs, reward, dones, infos = eval_env.step(action)\n",
    "\n",
    "    info = infos[0]\n",
    "    y = info[\"y_sample\"]\n",
    "    pred = info[\"pred\"]\n",
    "\n",
    "    if y == 1:\n",
    "        TA += 1\n",
    "        TP += 1 if pred == 1 else 0\n",
    "    else:\n",
    "        NO += 1\n",
    "        FP += 1 if pred == 1 else 0\n",
    "        TN += 1 if pred == 0 else 0\n",
    "\n",
    "TPR = TP / TA if TA > 0 else 0\n",
    "FPR = FP / NO if NO > 0 else 0\n",
    "\n",
    "print(\"\\n===== Evaluación rápida =====\")\n",
    "print(f\"TPR: {TPR*100:.2f}%\")\n",
    "print(f\"FPR: {FPR*100:.2f}%\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tesis-roberto",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
